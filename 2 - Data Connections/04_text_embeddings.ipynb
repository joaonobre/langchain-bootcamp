{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "728f1747-b8fc-4d31-96c2-047fc83c079d",
   "metadata": {},
   "source": [
    "#  Text Embeddings\n",
    "\n",
    "Langchain supports many different text embeddings that convert a text string to an embedded vectorized representation.\n",
    "\n",
    "We can store those embeddings vectors and perform similarity searches between new vectorized strings or documents against a vector store.\n",
    "\n",
    "We need to choose our embedding model very carefully because different embedding models cannot interact with each other. This means that it's not possible to calculate the cosine similarity between two embeddings vectors that were obtained with different embedding models. If we have a vector store that was obtained with a given embedding model and we want to change the embedding model for future data, we would need to re-embed the entire historical vectorized documents. This also means that we have to have access to the raw historical data because we can not go from embedding vector to original string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "244f42d8-2349-446e-a1c2-ae4b8d3ee38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d1c53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(key=\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b637a11-23a2-41e9-9689-09787b090df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e04ed710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'text-embedding-ada-002'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "752bb0b8-171c-4fa4-ad6d-81feef54beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Some normal text to send to OpenAI to be embedded into a vector\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b44a71e9-613a-427f-ba25-0373a0c27cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_text = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553ff223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd201025-120f-484e-b6b0-3ac92f80ab72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embedded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd9d15-de49-4023-ab0d-4bde26ce4874",
   "metadata": {},
   "source": [
    "## Embed Documents\n",
    "\n",
    "Instead of strings we can embed entire documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e64501be-11c7-464c-b316-aa2bc703e131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import CSVLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10e35048-eb5c-4660-bcee-be3b5334991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader('penguins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "681c0808-6524-4e54-bca4-b1f5da5476e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "578c5d01-b4bc-4908-9454-962b28bdc4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b60904e-23e5-4451-814d-3338f685797c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b817094e-d47e-4ca2-88fc-037cd6293895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable text contains each page_content for each row of the document list\n",
    "embedded_docs = embeddings.embed_documents([text.page_content for text in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cce4f482-ae72-4c19-9f76-376ea24378cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1f2c222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedded_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbc62332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.012749632519686768,\n",
       " -0.010540504691173527,\n",
       " -0.02000338536990946,\n",
       " -0.030577565753264004,\n",
       " -0.006903525279011112,\n",
       " 0.035265228810383714,\n",
       " -0.0368547244615598,\n",
       " -0.0006945620243483338,\n",
       " -0.01399563499302944,\n",
       " -0.03289445784566611]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedded_docs[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebb76c",
   "metadata": {},
   "source": [
    "We have 344 rows of vectors with 1536 dimensions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
